apply from: "$rootDir/gradle/base-teamscale.gradle"

import groovy.transform.Canonical
import groovy.json.JsonBuilder
import groovy.json.JsonSlurper
import java.time.Instant

// (TP)
// TODO: Make Task for getting all info from build (Tests, Findings, Build Result etc.)
// TODO: Sessions for uploading non-code and findings

@Canonical
class Finding {
	String findingTypeId
	String message
    String path
    String startLine
    String endLine
    String endColumn
    String startOffSet
    String endOffSet
}


@Canonical
class RoslynReport {
    List<Run> runs = [new Run()]
    String version = "1.0"

    public boolean addFinding(Finding finding) {
        return runs[0].results.add(new RoslynResult(finding))
    }

    public void addFindings(LinkedHashSet<Finding> findings) {
        findings.each { addFinding(it) }
    }

    public static RoslynReport getRulesReport(Map rules) {
        def report = new RoslynReport()
        report.runs.get(0).rules = rules
        return report
    }

    static class Run {
        Object tool = ["name": "roslyn"]
        List<RoslynResult> results = []
        Map rules = [:]
    }

    class RoslynResult {
        String ruleId
        String message
        Object locations

        public RoslynResult(Finding finding) {
            ruleId = finding.findingTypeId
            message = finding.message
            locations = [[
                "analysisTarget": [
                    "uri": finding.path,
                    "region": [
                        "startLine": finding.startLine,
                        "endLine": finding.endLine,
                        // TODO: parse if possible
                        "startColumn": finding.startOffSet ?: 1,
                        "endColumn": finding.endOffSet ?: 10000
                    ]
                ]
            ]]
        }
    }
}

// TODO find a way to general import buildscript deps...
buildscript {
	dependencies {
		classpath fileTree("$rootDir/gradle/lib").include("**.jar")
	}
}

ext {
	vsts = [
		// Map of VSTS/TFS URLs to login credentials if NTLM should not be used
		credentials: [
			// E.g. Basic AUTH:
			// 'https://my-account.visualstudio.com': [
			// 	type: 'basic',
			// 	username: 'foo',
			// 	password: 'bar'
			// ]
		],

		// Map of VSTS builds to process
		builds: [
			// Sample:
			// 'https://my-account.visualstudio.com': [
			// 	'MyProject': [
			// 		'MyBuild': [
			// 			logs: true
			// 		]
			// 	]
			// ]
		],

        loganalyzer: [] as Set,
		availableLoganalyzer: [
			msbuild: [
                //pattern: /(?:[\d\s\.:]*\d+>)?\s*(.*)\(([0-9]+)(?:,[0-9]+)?\).*warning[:\s]*((?:CS|SCS|SEC)\w*):\s?(.+?)\s+(?:\[([^\[]+)\]$)/,
                pattern: /\w+.\s+(.*?)\(([0-9]+),[0-9]+\):\s+warning\s+(\w+):\s+(.*?)\s\[(.*?)\]$/,
                convert: { matches ->
                    def match = matches[0]
                    def path = match[1].replace('\\', '/')
                    def project = match[5].replace('\\', '/').split('/').dropRight(1).join('/') // removes the csproject
                    if (!path.startsWith(project)) {
                        path = "$project/$path"
                    }

                    return new Finding(findingTypeId: match[3], message: match[4], path: path, startLine: match[2], endLine: match[2])
                }
			]
		],

		cacheFile: {
			def file = new File("$rootDir/cache/${project.name}_vsts_cache.json")
			file.getParentFile().mkdirs()
			return file
		}(),

		httpClient: { url, credentials ->
			return groovyx.net.http.OkHttpBuilder.configure {
				request.uri = url
				request.headers['Authorization'] = "Basic " + "${credentials.username}:${credentials.password}".bytes.encodeBase64().toString()
			}
		},

        availableRoslynReportRules: ["src/roslynRules/scs.json"] as HashSet
	]

    DEBUG = [
        TIMESTAMP: false,
		DISABLE_CACHE: false
    ]
}

def buildDefinitions = [:]
def cache = [
	lastProcessedCompletedTime: [:],
    uploadedRoslynRules: [] as HashSet
]

// ### Methods
/** Reads the content of the cache file if it exists */
def get_cache = {
    if (vsts.cacheFile?.exists() && vsts.cacheFile.text) {
        cache = new JsonSlurper().parseText(vsts.cacheFile.text)
    }
    return cache
}

/** Writes the cache to the project file */
def write_cache = {
    if (vsts.cacheFile && !DEBUG.DISABLE_CACHE) {
        vsts.cacheFile.write(new JsonBuilder(cache).toPrettyString())
    }
}

/** Appends a suffix to the partition if the project has more than one definition */
def appendPartitionName(partition, divider, definition, numberOfDefinitions) {
    if(numberOfDefinitions > 1) {
        assert definition.options.partition : "The project '$definition.project' has $numberOfDefinitions definitions, but " +
        "there is no partition defined for '$definition.name'.\n" +
        "If there are multiple definitions for a project, each one has to have a 'partition' property in its config gradle-file"
        partition += divider + definition.options.partition
    }

    return partition
}

/** Calculates the differences between two dates */
def getExecutionTime(startDate, endDate) {
    def start = Instant.parse(startDate).toEpochMilli()
    def end = Instant.parse(endDate).toEpochMilli()
    return (end - start) / 1000
}

/** Sets the 't' parameter for the request */
def setRequestTimeParameter(build, request) {
    def branch = build.targetBranch
    def timestamp = build.queueTime.toEpochMilli()

    if(DEBUG.TIMESTAMP) {
        timestamp = "HEAD"
    }

    def time = (branch ? branch + ":" : "") + timestamp

    if(time) {
        request.uri.query.t = time
    }
    return request
}


/** Uploads the content of a report to the teamscale instance */
def uploadReportContent(http, partition, type, content, build) {
    return http.post {
        request.uri.path = "/p/$teamscale.project/external-report"
        request.contentType = "multipart/form-data"
        request.uri.query = [
            "format": type,
            "message": "External Analysis ($type)",
            "partition": partition,
        ]
        setRequestTimeParameter(build, request)

        request.body = groovyx.net.http.MultipartContent.multipart {
            field "report", content
        }

        request.encoder "multipart/form-data", groovyx.net.http.OkHttpEncoders.&multipart
        response.failure(failure)
    }
}

/** Prints the standard upload message. See 'http.get' instances for usage examples */
def printBuildUpload(message, definition, build) {
    println " Uploading $message to $teamscale.project for $definition.name:$build.details.buildNumber"
    print   "     -> "
}

// ### Tasks
/** Collects all build definitions for the project */
task collectBuildDefinitions {
    def defaultOptions = [
        // default
        "branchMapping": { it }
    ]

	doLast {
		vsts.builds.each { vstsUrl, vstsProjects ->
			vstsProjects.each { vstsProjectName, vstsBuilds ->
				def http = vsts.httpClient(vstsUrl, vsts.credentials[vstsUrl])

				def response = http.get {
					request.uri.path = "/$vstsProjectName/_apis/build/definitions"
					request.uri.query = [
						'api-version': '4.1',
						'includeLatestBuilds': true
					]
                    response.failure(failure)
				}

				response.value.each { vstsBuildDefinition ->
					if (!vstsBuilds.containsKey(vstsBuildDefinition.name)) {
						return
					}

                    def lastCompletedTime = Instant.EPOCH
                    if(!vstsBuildDefinition.latestCompletedBuild) {
                        println " No build run/completed for $vstsBuildDefinition.name"
                    } else {
                        lastCompletedTime = Instant.parse(vstsBuildDefinition.latestCompletedBuild.finishTime)
                    }

					def buildDefinitionKey = [
                        url: vstsUrl,
                        project: vstsProjectName,
                        name: vstsBuildDefinition.name,
                        options: defaultOptions + vstsBuilds[vstsBuildDefinition.name]
                    ]

					buildDefinitions[buildDefinitionKey] = [
						id: vstsBuildDefinition.id,
						lastCompletedTime: lastCompletedTime,
						http: http,
						builds: []
					]
				}
			}
		}
	}
}

/**
 * Collects all new builds for every build definition.
 * A build is new, if its timestamp is bigger than the last cached one.
 */
task collectNewBuilds(dependsOn: collectBuildDefinitions) {
    /**
     * Checks if the build should be included based on the branchname.
     * Pull request, for example, should automatically be excluded.
     */
    def includeBuild = { branchName ->
        return !(branchName ==~ /^refs\/pull/)
    }

    /** Formats the branch to remove unneeded prefixes. */
    def formatBranchName = { branchName ->
        def gitRef = ~/^refs\/heads\//
        if (branchName.startsWith("\$")) {
            // TFS path will be ignored completely (for now)
            return ""
        }

        return branchName - gitRef
    }

	doLast {
        get_cache()

		buildDefinitions.each { definition, data ->
			def cacheKey = "$definition.url/$definition.project/$definition.name"
			def lastProcessedCompletedTime = cache.lastProcessedCompletedTime[cacheKey]
			if (!lastProcessedCompletedTime) {
				lastProcessedCompletedTime = Instant.EPOCH
			} else {
				lastProcessedCompletedTime = Instant.parse(lastProcessedCompletedTime)
				if (data.lastCompletedTime <= lastProcessedCompletedTime) {
					println "No new builds for $definition.name since $lastProcessedCompletedTime"
					return
				}
			}

			def response = data.http.get {
				request.uri.path = "/$definition.project/_apis/build/builds"
				request.uri.query = [
					'api-version': '4.1',
					'definitions': data.id,
					'minTime': lastProcessedCompletedTime.plusNanos(1),
					'status': 'completed',
					'queryOrder': 'finishTimeAscending',
				]
                response.failure(failure)
			}

			response.value.each { build ->
                def branchName = build.sourceBranch
                // Filter which builds we want to include
                if (!includeBuild(branchName)) {
                    return
                }

                branchName = formatBranchName(branchName)
                def targetBranch = definition.options.branchMapping(branchName)

                if(targetBranch instanceof String) {
                    data.builds += [
                        details: build,
                        targetBranch: targetBranch,
                        queueTime: Instant.parse(build.queueTime),
                        findings: new HashSet<Finding>()
                    ]
                }
			}

			// TODO update step by step?
			cache.lastProcessedCompletedTime[cacheKey] = data.lastCompletedTime.toString()
		}

        write_cache()
	}
}

/**
 * Fetches the test results for each build.
 * If the build does not run tests, the list is empty.
 */
task getTestResults(dependsOn: collectNewBuilds) {
    doLast {
		buildDefinitions.each { definition, definitionData ->
			definitionData.builds.each { build ->
                def testRuns = definitionData.http.get {
                    request.uri.path = "/$definition.project/_apis/test/Runs"
                    request.uri.query = [
                        "api-version" : "4.1",
                        "builduri" : "vstfs:///Build/Build/$build.details.id"
                    ]
                    response.failure(failure)
                }

                def results = []
                testRuns.value.each { run ->
                    results.add(definitionData.http.get {
                        request.uri.path = "/$definition.project/_apis/test/runs/$run.id"
                        request.uri.query = [:]
                    })
                    response.failure(failure)
                }

                build.testResult = results
            }
        }
    }
}

/**
 * Fetches and parses the logs for each build. Every found finding is added to the build object in build definitions.
 *
 * Only the logs which actually contain warning are being parsed.
 * If a log is longer than #maxFetchedLogLines it is fetched in batches
 */
task parseBuildLogs(dependsOn: collectNewBuilds) {
    /** Checks if the log in this record should be downloaded */
    def includeLogOfRecord = {definition, record ->
        if(!record.log || !(record.type ==~ /Task|Job/)) {
            return false
        }

        if(definition.options.taskNamePattern) {
            return (record.name ==~ definition.options.taskNamePattern)
        }

        // If we would include every task, take the job logs instead, as it contains the log of every
        // task, thus reducing the number of calls to the server
        return record.type == "Job"
    }

    /** Parses the given log with all specified analyzer */
    def parseLog = { log ->
        def findings = [] as Set
        vsts.loganalyzer.each { type ->
            def analyzer = vsts.availableLoganalyzer[type]
            log.eachLine { line ->
                if (line =~ analyzer.pattern) {
                    findings += analyzer.convert(java.util.regex.Matcher.lastMatcher)
                }
            }
        }
        return findings
    }

    def maxFetchedLogLines = 10000

	doLast {
		buildDefinitions.each { definition, definitionData ->
			definitionData.builds.each { build ->
                build.findings = [] as Set

                // Get the information of all logs (lineCount only exists here)
				def logs = definitionData.http.get {
					request.uri.path = "/$definition.project/_apis/build/builds/$build.details.id/logs"
					request.uri.query = [ 'api-version': '4.1' ]
                    response.failure(failure)
				}.value.groupBy { it.id }

                // Get the timeline ( for error- and warningCount )
                def timeline = definitionData.http.get {
					request.uri.path = "/$definition.project/_apis/build/builds/$build.details.id/timeline"
					request.uri.query = [ 'api-version': '4.1' ]
                    response.failure(failure)
                }

                // Get a map with id and linecount for logs
                def logRefs = timeline.records.collectMany {
                    if(includeLogOfRecord(definition, it)) {
                        return [ it.log.subMap("id") +
                                 it.subMap("name") +
                                 // groupBy creates a list. We are grouping by id, so there always is only the one element
                                 logs[it.log.id][0].subMap("lineCount") ]
                    }
                    return []
                }
                println " Parsing the logs of the following tasks in $definition.name:$build.details.buildNumber: " + logRefs.name

                // Parse the selected job logs
                logRefs.each { logRef ->
                    // Get Log in batches
                    def currentLine = 0
                    while(currentLine < logRef.lineCount) {
                        def log = definitionData.http.get {
                            request.uri.path = "/$definition.project/_apis/build/builds/$build.details.id/logs/$logRef.id"
                            request.uri.query = [
                                "api-version": "4.1",
                                "startLine": currentLine,
                                "endLine": currentLine + maxFetchedLogLines
                            ]
                            response.failure(failure)
                        }

                        build.findings += parseLog(log)

                        // endline is inclusiv. Therefore we need to add 1
                        currentLine += maxFetchedLogLines + 1
                    }
                }
			}
		}
	}
}

/**
 * Takes the roslyn rules, defined in the files provided by vsts.availableRoslynReportRules and uploads them
 * to the teamscale service via the `custom-roslyn-findings`
 *
 * TODO (BETA): the service is not usable at the moment (15.06.18), because of a bug in the custom-roslyn-findings service
 */
task uploadRoslynRules() {
    doLast {
        // check cache
        get_cache()

        // upload rules
        def diff = vsts.availableRoslynReportRules - cache.uploadedRoslynRules
        if(diff.size() > 0) {
            // get rules
            def http = teamscale.httpClient(teamscale)
            diff.each { rulesPath ->
                def rulesFile = new File("$rootDir/$rulesPath")
                def rulesJson = new JsonSlurper().parse(rulesFile)
                def report = new JsonBuilder(RoslynReport.getRulesReport(rulesJson)).toString()

                // Get Analysis Profile of project
                def analysisProfile = http.get {
                    request.uri.path = "/create-project/$teamscale.project"
                    request.contentType = "text/xml;charset=utf-8"
                    response.failure(failure)
                }.profile

                // upload
                println " Uploading '$rulesPath' to '$analysisProfile' for '$teamscale.project'"
                println "     -> " + http.post {
                    request.uri.path = "/custom-roslyn-findings"
                    request.contentType = "multipart/form-data"
                    request.uri.query = [
                        "analysis-profile": analysisProfile
                    ]

                    request.body = groovyx.net.http.MultipartContent.multipart {
                        field "report", report
                    }

                    request.encoder "multipart/form-data", groovyx.net.http.OkHttpEncoders.&multipart

                    response.failure(failure)
                    response.when("400") { response, reader ->
                        if(reader.contains("You did not provide any reports") && report.size() > 0) {
                            // Bug in the service. If the report was already uploaded, it reports an missing data error
                            return "'$analysisProfile' already contains the rules in '$rulesPath'"
                        } else {
                            failure(response, reader)
                        }
                    }
                }

                cache.uploadedRoslynRules += rulesPath
            }
        }

        // set cache
        write_cache()
    }
}

/** Gets the information for each artifact of a build, such that that parts can be downloaded */
task getArtifacts(dependsOn: collectNewBuilds) {
    def extractContainerId = { artifact ->
        def match = artifact.resource.data =~ /^\#\/([0-9]+)\/${artifact.name}$/
        if(match.find()) {
            return match.group(1)
        }
        return false;
    }

    doLast {
		def http = teamscale.httpClient(teamscale)
        buildDefinitions.each { definition, data ->
            data.builds.sort{ it.queueTime }.each { build ->
                def artifacts = data.http.get {
                    request.uri.path = "/$definition.project/_apis/build/builds/$build.details.id/artifacts"
                    request.uri.query = [
                        "api-version": "4.1"
                    ]
                    response.failure(failure)
                }.value.collectEntries {
                    def id = extractContainerId(it)
                    if(id) {
                        return [it.name, id]
                    }
                    return []
                }

                build.artifacts = artifacts
            }
        }
    }
}

/**
 * Fetches and uploads external analysis tool report (e.g. FindBugs) to teamscale.
 * This requires some configuration in order to make it work, see the sample file samples/vsts/project.gradle
 */
task uploadReports(dependsOn: getArtifacts) {
	doLast {
		def http = teamscale.httpClient(teamscale)
		buildDefinitions.each { definition, data ->
			// Check if reports are configured
			if(!definition.options.reports) {
				println " No reports configured for $definition.name"
				return
			}

			data.builds.sort{ it.queueTime }.each { build ->
				definition.options.reports.each { reportType, filter ->
					def partition = appendPartitionName("$reportType", ": ", definition, buildDefinitions.size())

					// Get files, which should be downloaded
					def artifacts = build.artifacts.findAll { k,v -> k ==~ /${filter.artifact}/ }
					def files = artifacts.collectMany { name, id ->
						return data.http.get {
							request.uri.path = "/_apis/resources/Containers/$id"
							request.uri.query = [
								"itemPath": name
							]
							response.failure(failure)
						}.value.collectMany {
							def fileName = it.path.split("/").last()
							(it.itemType == "file" && fileName ==~ /${filter.file}/ ? [it.subMap("path", "contentLocation")] : [])
						}
					}

					files.each { file ->
						// Download report
						def fileName = file.path.split("/").last()
						File tmpFile = new File("$rootDir/tmp/" + fileName)
						tmpFile.getParentFile().mkdirs()

						def content = data.http.get {
							request.uri = file.contentLocation
							groovyx.net.http.optional.Download.toFile(delegate, tmpFile)
							response.failure(failure)
						}

						try {
							// Upload report to teamscale
							printBuildUpload("'$fileName'", definition, build)
							println uploadReportContent(http, partition, reportType, tmpFile.text, build)
						} finally {
							tmpFile.delete()
						}
					}
				}
			}
		}
	}
}

/**
 * Uploads the test results for every definition to the teamscale server as a non-code metric.
 * If project has multiple definitions, each definition has to define a separate partition to which the
 * results will be uploaded to.
 */
task uploadTestResults(dependsOn: getTestResults) {
	doLast {
		def http = teamscale.httpClient(teamscale)
		buildDefinitions.each { definition, data ->
			data.builds.sort{ it.queueTime }.each { build ->
				if(build.testResult.size() < 1) {
					println " No tests run for build $definition.name:$build.details.buildNumber"
					return
				}

				// Aggregate if there are multiple runs in one build
				def result = ["passed": 0, "total": 0, "time": 0]
				build.testResult.each { run ->
					result.passed += run.passedTests
					result.total += run.totalTests
					result.time += getExecutionTime(run.startedDate, run.completedDate)
				}

				// Upload the result
				def partition = appendPartitionName("Test", ": ", definition, buildDefinitions.size())
				def path = appendPartitionName("Test Success", "/", definition, buildDefinitions.size())

				printBuildUpload("test results", definition, build)
				println http.put {
					request.uri.path = "/p/$teamscale.project/add-non-code-metrics"
					request.contentType = "application/json"
					request.uri.query = [
						"skip-session": true,
						"message": "External Analysis ($partition)",
						"partition": partition
					]
					setRequestTimeParameter(build, request)

					request.body = [[
										"path": path,
										"content": "$result.passed/$result.total tests passed",
										"time": result.time,
										"assessment" : [
											"GREEN" : result.passed,
											"RED": result.total - result.passed
										]
									]]
					response.failure(failure)
				}
			}
		}
	}
}

/**
 * Uploads the build result for every definition to the teamscale server as a non-code metric.
 * If project has multiple definitions, each definition has to define a separate partition to which the
 * results will be uploaded to.
 */
task uploadBuildStatus(dependsOn: collectNewBuilds) {
	def buildResultMap = [
		"failed" : [
			"assessment": "RED",
			"content": "Build is unstable"
		],
		"succeeded": [
			"assessment": "GREEN",
			"content": "Build is stable"
		]
	]

	doLast {
		def http = teamscale.httpClient(teamscale)

		buildDefinitions.each { definition, data ->
			data.builds.sort{ it.queueTime }.each { build ->
				if(build.details.status != "completed") {
					return
				}

				def buildResult = buildResultMap[build.details.result]
				def partition = appendPartitionName("Build", ": ", definition, buildDefinitions.size())

				printBuildUpload("build status", definition, build)
				println http.put {
					request.uri.path = "/p/$teamscale.project/add-non-code-metrics"
					request.contentType = "application/json"
					request.uri.query = [
						"skip-session": true,
						"message": "External Analysis ($partition)",
						"partition": appendPartitionName("Build", ": ", definition, buildDefinitions.size())
					]
					setRequestTimeParameter(build, request)

					request.body = [[
										"path": appendPartitionName("Build Stability", "/", definition, buildDefinitions.size()),
										"content": buildResult.content,
										"time": getExecutionTime(build.details.startTime, build.details.finishTime),
										"assessment" : [ "$buildResult.assessment" : 1 ]
									]]

					response.failure(failure)
				}
			}
		}
	}
}

/**
 * Uploads the findings which are reported during the build to the teamscale server.
 * See #parseBuildLogs for more details on which findings are created.
 */
task uploadBuildFindings(dependsOn: parseBuildLogs) {
	doLast {
		def http = teamscale.httpClient(teamscale)
		buildDefinitions.each { definition, data ->
			data.builds.sort{ it.queueTime }.each { build ->
				def partition = appendPartitionName("Findings", ": ", definition, buildDefinitions.size())

				// Upload findings
				printBuildUpload("${build.findings.size()} finding(s)", definition, build)
				println http.put {
					request.uri.path = "/p/$teamscale.project/add-external-findings"
					request.contentType = 'application/json'
					request.uri.query = [
						"message": "External Analysis ($partition)",
						"partition": partition,
						"skip-session": "true"
					]
					setRequestTimeParameter(build, request)

					def fileFindings = build.findings.groupBy({ finding -> finding.path }).collect { k, v ->
						[ "path" : k, "findings" : v ]
					}

					request.body = fileFindings
					response.failure(failure)
				}
			}
		}
	}
}

/**
 * Converts the findings to a Roslyreport and uploads it
 *
 * TODO (BETA): Depends on uploadRoslynRules, which is not working correctly at the moment
 */
task uploadFindingsAsRoslynReport(dependsOn: parseBuildLogs) {
	doLast {
		def http = teamscale.httpClient(teamscale)
		buildDefinitions.each { definition, data ->
			data.builds.sort{ it.queueTime }.each { build ->
				// TODO: Check if the findings are in fact C# Findings
				if(!build.findings) {
					return
				}

				// Create the report
				def report = new RoslynReport();
				report.addFindings(build.findings)
				pprint(report)
				report = new JsonBuilder(report).toString()

				def partition = appendPartitionName("Roslyn Report", ": ", definition, buildDefinitions.size())

				// upload
				printBuildUpload("${build.findings.size()} roslyn findings", definition, build)
				println uploadReportContent(http, partition, "roslyn", report, build)
			}
		}
	}
}
