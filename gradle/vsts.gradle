apply from: "$rootDir/gradle/base-teamscale.gradle"

import groovy.transform.Canonical
import groovy.json.JsonBuilder
import groovy.json.JsonSlurper
import java.time.Instant

@Canonical
class Finding {
	String findingTypeId
	String message
    String path
    String startLine
    String endLine
    String endColumn
    String startOffSet
    String endOffSet
}


@Canonical
class RoslynReport {
    List<Run> runs = [new Run()]
    String version = "1.0"

    public boolean addFinding(Finding finding) {
        return runs[0].results.add(new RoslynResult(finding))
    }

    public boolean addFindings(LinkedHashSet<Finding> findings) {
        findings.each { addFinding(it) }
    }

    @Canonical
    static class Run {
        Object tool = ["name": "roslyn"]
        List<RoslynResult> results = []
        List<Object> rules = []
    }

    @Canonical
    class RoslynResult {
        String ruleId
        String message
        Object locations

        public RoslynResult(Finding finding) {
            ruleId = finding.findingTypeId
            message = finding.message
            locations = [[
                "analysisTarget": [
                    "uri": finding.path,
                    "region": [
                        "startLine": finding.startLine,
                        "endLine": finding.endLine,
                        // TODO: parse if possible
                        "startColumn": finding.startOffSet ?: 1,
                        "endColumn": finding.endOffSet ?: 10000
                    ]
                ]
            ]]
        }
    }
}

// (TP)
// TODO: Make Task for getting all info from build (Tests, Findings, Build Result etc.)
// TODO: Sessions for uploading non-code and findings
// TODO: Roslyn Report is possible. Also you can update the roslyn rules with "custom-roslyn-findings" service
// TODO: Better data management

// TODO find a way to general import buildscript deps...
buildscript {
	dependencies {
		classpath fileTree("$rootDir/gradle/lib").include("**.jar")
	}
}

ext {
	vsts = [
		// Map of VSTS/TFS URLs to login credentials if NTLM should not be used
		credentials: [
			// E.g. Basic AUTH:
			// 'https://my-account.visualstudio.com': [
			// 	type: 'basic',
			// 	username: 'foo',
			// 	password: 'bar'
			// ]
		],

		// Map of VSTS builds to process
		builds: [
			// Sample:
			// 'https://my-account.visualstudio.com': [
			// 	'MyProject': [
			// 		'MyBuild': [
			// 			logs: true
			// 		]
			// 	]
			// ]
		],

        loganalyzer: [] as Set,
		availableLoganalyzer: [
			msbuild: [
                //pattern: /(?:[\d\s\.:]*\d+>)?\s*(.*)\(([0-9]+)(?:,[0-9]+)?\).*warning[:\s]*((?:CS|SCS|SEC)\w*):\s?(.+?)\s+(?:\[([^\[]+)\]$)/,
                pattern: /\w+.\s+(.*?)\(([0-9]+),[0-9]+\):\s+warning\s+(\w+):\s+(.*?)\s\[(.*?)\]$/,
                convert: { matches ->
                    def match = matches[0]
                    def path = match[1].replace('\\', '/')
                    def project = match[5].replace('\\', '/').split('/').dropRight(1).join('/') // removes the csproject
                    if (!path.startsWith(project)) {
                        path = "$project/$path"
                    }

                    return new Finding(findingTypeId: match[3], message: match[4], path: path, startLine: match[2], endLine: match[2])
                }
			]
		],

		cacheFile: file("${project.name}_vsts_cache.json"),

		httpClient: { url, credentials ->
			return groovyx.net.http.OkHttpBuilder.configure {
				request.uri = url
				request.headers['Authorization'] = "Basic " + "${credentials.username}:${credentials.password}".bytes.encodeBase64().toString()
			}
		}
	]
}

def buildDefinitions = [:]
def cache = [
	lastProcessedCompletedTime: [:]
]

// ### Methods
def pprint = { input ->
    println new JsonBuilder(input).toPrettyString()
}

/** Appends a suffix to the partition if the project has more than one definition */
def appendPartitionName = { partition, divider, definition, numberOfDefinitions ->
    if(numberOfDefinitions > 1) {
        assert definition.options.partition : "The project '$definition.project' has $numberOfDefinitions definitions, but " +
        "there is no partition defined for '$definition.name'.\n" +
        "If there are multiple definitions for a project, each one has to have a 'partition' property in its config gradle-file"
        partition += divider + definition.options.partition
    }

    return partition
}

/** Calculates the differences between two dates */
def getExecutionTime = { startDate, endDate ->
    def start = Instant.parse(startDate).toEpochMilli()
    def end = Instant.parse(endDate).toEpochMilli()
    return (end - start) / 1000
}

/**
 * Returns the timestamp of the build.
 * If the definition defines a branch, it is added to the timestapm */
def getBuildTimestamp = { definition, build ->

    return (definition.options.branch ? definition.options.branch + ":" : "") + build.queueTime.toEpochMilli()
}

/**
 * Uploads the given report.
 * http is a HTTP-Client for a teamscale
 */
def uploadReport = { http, partition, type, report, t ->
    uploadReportContent(http, partition, type, report.text, t)
}

def uploadReportContent = { http, partition, type, content, t ->
    return http.post {
        request.uri.path = "/p/$teamscale.project/external-report"
        request.contentType = "multipart/form-data"
        request.uri.query = [
            //"t": t,
            "format": type,
            "message": "External Analysis ($type)",
            "partition": partition,
        ]

        request.body = groovyx.net.http.MultipartContent.multipart {
            field "report", content
        }

        request.encoder "multipart/form-data", groovyx.net.http.OkHttpEncoders.&multipart
    }
}

// ### Tasks
/**
 * Collects all build definitions for the project
 */
task collecBuildDefinitions {
	doLast {
		vsts.builds.each { vstsUrl, vstsProjects ->
			vstsProjects.each { vstsProjectName, vstsBuilds ->
				def http = vsts.httpClient(vstsUrl, vsts.credentials[vstsUrl])
				def response = http.get {
					request.uri.path = "/$vstsProjectName/_apis/build/definitions"
					request.uri.query = [
						'api-version': '4.1',
						'includeLatestBuilds': true
					]
				}

				response.value.each { vstsBuildDefinition ->
					if (!vstsBuilds.containsKey(vstsBuildDefinition.name)) {
						return;
					}

                    def lastCompletedTime = Instant.EPOCH
                    if(!vstsBuildDefinition.latestCompletedBuild) {
                        println " No build run/completed for $vstsBuildDefinition.name"
                    } else {
                        lastCompletedTime = Instant.parse(vstsBuildDefinition.latestCompletedBuild.finishTime)
                    }

					def buildDefinitionKey = [
                        url: vstsUrl,
                        project: vstsProjectName,
                        name: vstsBuildDefinition.name,
                        options: vstsBuilds[vstsBuildDefinition.name]
                    ]

					buildDefinitions[buildDefinitionKey] = [
						id: vstsBuildDefinition.id,
						lastCompletedTime: lastCompletedTime,
						http: http,
						builds: []
					]
				}
			}
		}
	}
}

/**
 * Collects all new builds for every build definition.
 * A build is new, if its timestamp is bigger than the last cached one.
 */
task collectNewBuilds(dependsOn: collecBuildDefinitions) {
	doLast {
		if (vsts.cacheFile?.exists() && vsts.cacheFile.text) {
			cache = new JsonSlurper().parseText(vsts.cacheFile.text)
		}

		buildDefinitions.each { definition, definitionData ->
			def cacheKey = "$definition.url/$definition.project/$definition.name"
			def lastProcessedCompletedTime = cache.lastProcessedCompletedTime[cacheKey]
			if (!lastProcessedCompletedTime) {
				lastProcessedCompletedTime = Instant.EPOCH
			} else {
				lastProcessedCompletedTime = Instant.parse(lastProcessedCompletedTime)
				if (definitionData.lastCompletedTime <= lastProcessedCompletedTime) {
					return
				}
			}

			def response = definitionData.http.get {
				request.uri.path = "/$definition.project/_apis/build/builds"
				request.uri.query = [
					'api-version': '4.1',
					'definitions': definitionData.id,
					'minTime': lastProcessedCompletedTime.plusNanos(1),
					'status': 'completed',
					'queryOrder': 'finishTimeAscending',
				]
			}

			response.value.each { build ->
				definitionData.builds += [
					details: build,
					queueTime: Instant.parse(build.queueTime),
					findings: new HashSet<Finding>()
				]
			}

			// TODO update step by step?
			cache.lastProcessedCompletedTime[cacheKey] = definitionData.lastCompletedTime.toString()
		}

		// TODO move after build info is uploaded
		if (vsts.cacheFile) {
			//vsts.cacheFile.write(new JsonBuilder(cache).toPrettyString())
		}
	}
}

/**
 * Fetches the test results for each build.
 * If the build does not run tests, the list is empty.
 */
task getTestResults(dependsOn: collectNewBuilds) {
    doLast {
		buildDefinitions.each { definition, definitionData ->
			definitionData.builds.each { build ->
                def testRuns = definitionData.http.get {
                    request.uri.path = "/$definition.project/_apis/test/Runs"
                    request.uri.query = [
                        "api-version" : "4.1",
                        "builduri" : "vstfs:///Build/Build/$build.details.id"
                    ]
                }

                def results = []
                testRuns.value.each { run ->
                    results.add(definitionData.http.get {
                        request.uri.path = "/$definition.project/_apis/test/runs/$run.id"
                        request.uri.query = [:]
                    })
                }

                build.testResult = results
            }
        }
    }
}

/**
 * Uploads the test results for every definition to the teamscale server as a non-code metric.
 * If project has multiple definitions, each definition has to define a separate partition to which the
 * results will be uploaded to.
 */
task uploadTestResults(dependsOn: getTestResults) {
    doLast {
		def http = teamscale.httpClient(teamscale)
        buildDefinitions.each { definition, data ->
            data.builds.sort{ it.queueTime }.each { build ->
                if(build.testResult.size() < 1) {
                    println " No tests run for build $definition.name:$build.details.buildNumber"
                    return
                }

                // Aggregate if there are multiple runs in one build
                def result = ["passed": 0, "total": 0, "time": 0]
                build.testResult.each { run ->
                    result.passed += run.passedTests
                    result.total += run.totalTests
                    result.time += getExecutionTime(run.startedDate, run.completedDate)
                }

                // Upload the result
                def partition = appendPartitionName("Test", ": ", definition, buildDefinitions.size())
                def path = appendPartitionName("Test Success", "/", definition, buildDefinitions.size())

                println " Uploading test results to $teamscale.project for $definition.name:$build.details.buildNumber"
                println "     -> " + http.put {
                    request.uri.path = "/p/$teamscale.project/add-non-code-metrics"
                    request.contentType = "application/json"
                    request.uri.query = [
                        "t": getBuildTimestamp(definition, build),
                        "skip-session": true,
                        "message": "External Analysis ($partition)",
                        "partition": partition
                    ]

                    request.body = [[
                        "path": path,
                        "content": "$result.passed/$result.total tests passed",
                        "time": result.time,
                        "assessment" : [
                            "GREEN" : result.passed,
                            "RED": result.total - result.passed
                        ]
                    ]]
                }
            }
        }
    }
}

/**
 * Uploads the build result for every definition to the teamscale server as a non-code metric.
 * If project has multiple definitions, each definition has to define a separate partition to which the
 * results will be uploaded to.
 */
task uploadBuildStatus(dependsOn: collectNewBuilds) {
    def buildResultMap = [
        "failed" : [
            "assessment": "RED",
            "content": "Build is unstable"
        ],
        "succeeded": [
            "assessment": "GREEN",
            "content": "Build is stable"
        ]
    ]

    doLast {
		def http = teamscale.httpClient(teamscale)

        buildDefinitions.each { definition, data ->
            pprint(data.builds.last().details)
            data.builds.sort{ it.queueTime }.each { build ->
                if(build.details.status != "completed") {
                    return
                }

                def buildResult = buildResultMap[build.details.result]
                def partition = appendPartitionName("Build", ": ", definition, buildDefinitions.size())

                println " Uploading build status to $teamscale.project for build $definition.name:$build.details.buildNumber"
                println "     -> " + http.put {
                    request.uri.path = "/p/$teamscale.project/add-non-code-metrics"
                    request.contentType = "application/json"
                    request.uri.query = [
                        "t": getBuildTimestamp(definition, build),
                        "skip-session": true,
                        "message": "External Analysis ($partition)",
                        "partition": appendPartitionName("Build", ": ", definition, buildDefinitions.size())
                    ]

                    request.body = [[
                        "path": appendPartitionName("Build Stability", "/", definition, buildDefinitions.size()),
                        "content": buildResult.content,
                        "time": getExecutionTime(build.details.startTime, build.details.finishTime),
                        "assessment" : [ "$buildResult.assessment" : 1 ]
                    ]]
                }
            }
        }
    }
}

/**
 * Fetches and parses the logs for each build. Every found finding is added to the build object in build definitions.
 *
 * Only the logs which actually contain warning are being parsed.
 * If a log is longer than #maxFetchedLogLines it is fetched in batches
 */
task parseBuildLogs(dependsOn: collectNewBuilds) {
    /** Checks if the log in this record should be downloaded */
    def includeLogOfRecord = {definition, record ->
        if(!record.log || !(record.type ==~ /Task|Job/)) {
            return false
        }

        if(definition.options.taskNamePattern) {
            return (record.name ==~ definition.options.taskNamePattern)
        }

        // If we would include every task, take the job logs instead, as it contains the log of every
        // task, thus reducing the number of calls to the server
        // TODO: Maybe also look for jobs with warnings first instead of returning the complete log?
        return record.type == "Job"
    }

    /** Parses the given log with all specified analyzer */
    def parseLog = { log ->
        def findings = [] as Set
        vsts.loganalyzer.each { type ->
            def analyzer = vsts.availableLoganalyzer[type]
            log.eachLine { line ->
                if (line =~ analyzer.pattern) {
                    findings += analyzer.convert(java.util.regex.Matcher.lastMatcher)
                }
            }
        }
        return findings
    }

    def maxFetchedLogLines = 10000

	doLast {
		buildDefinitions.each { definition, definitionData ->
			definitionData.builds.each { build ->
                build.findings = [] as Set

                // Get the information of all logs (lineCount only exists here)
				def logs = definitionData.http.get {
					request.uri.path = "/$definition.project/_apis/build/builds/$build.details.id/logs"
					request.uri.query = [ 'api-version': '4.1' ]
				}.value.groupBy { it.id }

                // Get the timeline ( for error- and warningCount )
                def timeline = definitionData.http.get {
					request.uri.path = "/$definition.project/_apis/build/builds/$build.details.id/timeline"
					request.uri.query = [ 'api-version': '4.1' ]
                }

                // Get a map with id and linecount for logs
                def logRefs = timeline.records.collectMany {
                    if(includeLogOfRecord(definition, it)) {
                        return [ it.log.subMap("id") +
                                 it.subMap("name") +
                                 // groupBy creates a list. We are grouping by id, so there always is only the one element
                                 logs[it.log.id][0].subMap("lineCount") ]
                    }
                    return []
                }
                println " Parsing the logs of the following tasks in $definition.name:$build.details.buildNumber: " + logRefs.name

                // Parse the selected job logs
                logRefs.each { logRef ->
                    // Get Log in batches
                    def currentLine = 0
                    while(currentLine < logRef.lineCount) {
                        def log = definitionData.http.get {
                            request.uri.path = "/$definition.project/_apis/build/builds/$build.details.id/logs/$logRef.id"
                            request.uri.query = [
                                "api-version": "4.1",
                                "startLine": currentLine,
                                "endLine": currentLine + maxFetchedLogLines
                            ]
                        }

                        build.findings += parseLog(log)

                        // endline is inclusiv. Therefore we need to add 1
                        currentLine += maxFetchedLogLines + 1
                    }
                }
			}
		}
	}
}

task convertFindingsToRoslynReport(dependsOn: parseBuildLogs) {
    doLast {
        def http = teamscale.httpClient(teamscale)
        buildDefinitions.each { definition, data ->
            data.builds.sort{ it.queueTime }.each { build ->
                // TODO: Check if the findings are in fact C# Findings
                if(!build.findings) {
                    return
                }

                // Create the report
                def report = new RoslynReport();
                report.addFinding(build.findings.first())
                pprint report
                report = new JsonBuilder(report).toString()

                def partition = appendPartitionName("Roslyn Report", ": ", definition, buildDefinitions.size())

                // upload
                println " Uploading ${build.findings.size()} roslyn findings to '$teamscale.project' for $definition.name:$build.details.buildNumber"
                println "     -> " + uploadReportContent(http, partition, "roslyn", report, getBuildTimestamp(definition, build))
            }
        }
    }
}

/**
 * Uploads the findings which are reported during the build to the teamscale server.
 * See #parseBuildLogs for more details on which findings are created.
 */
task uploadBuildFindings(dependsOn: parseBuildLogs) {
	doLast {
		def http = teamscale.httpClient(teamscale)
        buildDefinitions.each { definition, data ->
            data.builds.sort{ it.queueTime }.each { build ->
                def partition = appendPartitionName("Findings", ": ", definition, buildDefinitions.size())

                // Upload findings
                println " Uploading ${build.findings.size()} findings to '$teamscale.project' for $definition.name:$build.details.buildNumber"
                println "     -> " + http.put {
                    request.uri.path = "/p/$teamscale.project/add-external-findings"
                    request.contentType = 'application/json'
                    request.uri.query = [
                        // TODO: Uncomment
                        //"t": getBuildTimestamp(definition, build),
                        "message": "External Analysis ($partition)",
                        "partition": partition,
                        "skip-session": "true"
                    ]

                    def fileFindings = build.findings.groupBy({ finding -> finding.path }).collect { k, v ->
                        [ "path" : k, "findings" : v ]
                    }

                    request.body = fileFindings
                }
            }
        }
    }
}

/** Gets the information for each artifact of a build, such that that parts can be downloaded */
task getArtifacts(dependsOn: collectNewBuilds) {
    def extractContainerId = { artifact ->
        def match = artifact.resource.data =~ /^\#\/([0-9]+)\/${artifact.name}$/
        if(match.find()) {
            return match.group(1)
        }
        return false;
    }

    doLast {
		def http = teamscale.httpClient(teamscale)
        buildDefinitions.each { definition, data ->
            data.builds.sort{ it.queueTime }.each { build ->
                def artifacts = data.http.get {
                    request.uri.path = "/$definition.project/_apis/build/builds/$build.details.id/artifacts"
                    request.uri.query = [
                        "api-version": "4.1"
                    ]
                }.value.collectEntries {
                    def id = extractContainerId(it)
                    if(id) {
                        return [it.name, id]
                    }
                    return []
                }

                build.artifacts = artifacts
            }
        }
    }
}

/**
 * Fetches and uploads external analysis tool report (e.g. FindBugs) to teamscale.
 * This requires some configuration, see this template:
 * .... "{definiton}": [ "reports" : [ "{reportType}": [ "artifact": "{regex}", "file": "{regex}" ]]]
 * The reportType needs to be one of the EReportFormat.java
 * The artifact is the archive on the VSTS which contains the report file. In order to match both, you need to
 * specify regexes for both.
 */
task uploadReports(dependsOn: getArtifacts) {
    doLast {
		def http = teamscale.httpClient(teamscale)
        buildDefinitions.each { definition, data ->
            // Check if reports are configured
            if(!definition.options.reports) {
                println " No reports configured for $definition.name"
                return
            }

            data.builds.sort{ it.queueTime }.each { build ->
                definition.options.reports.each { reportType, filter ->
                    def partition = appendPartitionName("$reportType", ": ", definition, buildDefinitions.size())

                    // Get files, which should be downloaded
                    def artifacts = build.artifacts.findAll { k,v -> k ==~ /${filter.artifact}/ }
                    def files = artifacts.collectMany { name, id ->
                        return data.http.get {
                            request.uri.path = "/_apis/resources/Containers/$id"
                            request.uri.query = [
                                "itemPath": name
                            ]
                        }.value.collectMany {
                            def fileName = it.path.split("/").last()
                            (it.itemType == "file" && fileName ==~ /${filter.file}/ ? [it.subMap("path", "contentLocation")] : [])
                        }
                    }

                    files.each { file ->
                        // Download report
                        def fileName = file.path.split("/").last()
                        File tmpFile = new File("tmp/" + fileName)
                        tmpFile.getParentFile().mkdirs()

                        def content = data.http.get {
                            request.uri = file.contentLocation
                            groovyx.net.http.optional.Download.toFile(delegate, tmpFile)
                        }

                        try {
                            // Upload  report to teamscale
                            println " Uploading '$fileName' to $teamscale.project for $definition.name:$build.details.buildNumber"
                            println "     -> " + uploadReport(http, partition, reportType, tmpFile)
                        } finally {
                            tmpFile.delete()
                        }
                    }
                }
            }
        }
    }
}

