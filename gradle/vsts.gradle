apply from: "$rootDir/gradle/base-teamscale.gradle"

import groovy.transform.Canonical
import groovy.json.JsonBuilder
import groovy.json.JsonSlurper
import java.time.Instant

@Canonical
class Finding {
	String findingTypeId
	String message
	String path
	String startLine
	String endLine
	String endColumn
	String startOffSet
	String endOffSet
}


@Canonical
class RoslynReport {
	List<Run> runs = [new Run()]
	String version = "1.0"

	boolean addFinding(Finding finding) {
		return runs[0].results.add(new RoslynResult(finding))
	}

	void addFindings(LinkedHashSet<Finding> findings) {
		findings.each { addFinding(it) }
	}

	static RoslynReport getRulesReport(Map rules) {
		def report = new RoslynReport()
		report.runs.get(0).rules = rules
		return report
	}

	static class Run {
		Object tool = ["name": "roslyn"]
		List<RoslynResult> results = []
		Map rules = [:]
	}

	class RoslynResult {
		String ruleId
		String message
		Object locations

		RoslynResult(Finding finding) {
			ruleId = finding.findingTypeId
			message = finding.message
			locations = [[
							 "analysisTarget": [
								 "uri"   : finding.path,
								 "region": [
									 "startLine"  : finding.startLine,
									 "endLine"    : finding.endLine,
									 // TODO: parse if possible
									 "startColumn": finding.startOffSet ?: 1,
									 "endColumn"  : finding.endOffSet ?: 10000
								 ]
							 ]
						 ]]
		}
	}
}

// TODO find a way to general import buildscript deps...
buildscript {
	dependencies {
		classpath fileTree("$rootDir/gradle/lib").include("**.jar")
	}
}

ext {
	vsts = [
		// Map of VSTS/TFS URLs to login credentials if NTLM should not be used
		credentials               : [
			// E.g. Basic AUTH:
			// 'https://my-account.visualstudio.com': [
			// 	type: 'basic',
			// 	username: 'foo',
			// 	password: 'bar'
			// ]
		],

		// Map of VSTS builds to process
		builds                    : [
			// Sample:
			// 'https://my-account.visualstudio.com': [
			// 	'MyProject': [
			// 		'MyBuild': [
			// 			logs: true
			// 		]
			// 	]
			// ]
		],

		loganalyzer               : [] as Set,
		availableLoganalyzer      : [
			msbuild: [
				//pattern: /(?:[\d\s\.:]*\d+>)?\s*(.*)\(([0-9]+)(?:,[0-9]+)?\).*warning[:\s]*((?:CS|SCS|SEC)\w*):\s?(.+?)\s+(?:\[([^\[]+)\]$)/,
				pattern: /\w+.\s+(.*?)\(([0-9]+),[0-9]+\):\s+warning\s+(\w+):\s+(.*?)\s\[(.*?)\]$/,
				convert: { matches ->
					def match = matches[0]
					def path = match[1].replace('\\', '/')
					def project = match[5].replace('\\', '/').split('/').dropRight(1).join('/') // removes the csproject
					if (!path.startsWith(project)) {
						path = "$project/$path"
					}

					return new Finding(findingTypeId: match[3], message: match[4], path: path, startLine: match[2], endLine: match[2])
				}
			]
		],

		cacheFile                 : {
			def file = new File("$rootDir/cache/${project.name}_vsts_cache.json")
			file.getParentFile().mkdirs()
			return file
		}(),

		httpClient                : { url, credentials ->
			return groovyx.net.http.OkHttpBuilder.configure {
				request.uri = url
				// Workaround for a HTTPBuilder Problem/Bug
				teamscale.url = teamscale.url.replaceAll("/\$", "")
				teamscale.prefix = new URL(teamscale.url).getPath() ?: ""

				request.headers['Authorization'] = "Basic " + "${credentials.username}:${credentials.password}".bytes.encodeBase64().toString()
			}
		},

		availableRoslynReportRules: ["src/roslynRules/scs.json"] as HashSet
	]

	DEBUG = [
		TIMESTAMP    : false,
		DISABLE_CACHE: false
	]

	// Maximum number of lines of log which will be fetched in one go
	MAX_FETCHED_LOG_LINES = 10000

	// Maximum number of files which are allowed to be matched by a file regex.
	MAX_MATCHED_FILES = 10

	// Path to the code coverage exe
	CODE_COVERAGE_EXE = ""
}


def buildDefinitions = [:]
def cache = [
	lastProcessedCompletedTime: [:] //,
	//uploadedRoslynRules       : [] as HashSet
]

/** Reads the content of the cache file if it exists */
def getCache = {
	if (vsts.cacheFile?.exists() && vsts.cacheFile.text) {
		cache = new JsonSlurper().parseText(vsts.cacheFile.text)
	}
	return cache
}

/** Writes the cache to the project file */
def writeCache = {
	if (vsts.cacheFile && !DEBUG.DISABLE_CACHE) {
		vsts.cacheFile.write(new JsonBuilder(cache).toPrettyString())
	}
}

// ### Methods
/** Appends a suffix to the partition if the project has more than one definition */
def appendPartitionName(partition, divider, definition) {
	if (definition.options.partition) {
		partition += divider + definition.options.partition
	}
	return partition
}

/** Calculates the differences between two dates */
def getExecutionTime(startDate, endDate) {
	def start = Instant.parse(startDate).toEpochMilli()
	def end = Instant.parse(endDate).toEpochMilli()
	return (end - start) / 1000
}

/** Sets the 't' parameter for the request */
def setRequestTimeParameter(build, request) {
	def branch = build.targetBranch
	def timestamp = build.queueTime.toEpochMilli()

	if (DEBUG.TIMESTAMP) {
		timestamp = "HEAD"
	}

	def time = (branch ? branch + ":" : "") + timestamp

	if (time) {
		request.uri.query.t = time
	}
	return request
}

/** Uploads the content of a report to the teamscale instance */
def uploadExternalReport(http, type, content, definition, build, sessionId = null) {
	try {
		return http.post {
			request.uri.path = "$teamscale.prefix/p/$teamscale.project/external-report"
			request.contentType = "multipart/form-data"
			request.uri.query = [
				"format"   : type,
				"message"  : "External Analysis ($type)",
				"partition": appendPartitionName("Metrics", ": ", definition),
			]

			if (sessionId) {
				request.uri.query["session"] = sessionId
			}
			setRequestTimeParameter(build, request)

			request.body = groovyx.net.http.MultipartContent.multipart {
				field "report", content
			}

			request.encoder "multipart/form-data", groovyx.net.http.OkHttpEncoders.&multipart
			response.failure failure
		}
	} catch (e) {
		e.printStackTrace()
		return "failed"
	}
}

/** Uploads the given list of reports */
def uploadExternalReports(http, reports, definition, build) {
	def partition = appendPartitionName("Metrics", ": ", definition)

	if (!(reports.size() > 0)) {
		return
	}

	// Workaround with the session ids, because the `external-report` service does not provide method
	// to obtain a session-id
	def sessionId = http.get {
		request.uri.path = "$teamscale.prefix/p/$teamscale.project/add-external-findings"
		request.uri.query = [
			"partition": partition,
			"message"  : "External Analysis (${reports.keySet().join(",")})"
		]
		setRequestTimeParameter(build, request)
		response.failure failure
	}

	// Upload the reports
	reports.keySet().each { type ->
		if (reports.get(type).size() > 0) {
			log("Uploading ${reports.get(type).size()} $type report(s)", definition, build)
		}
		reports.get(type).each { content ->
			println "    -> " + uploadExternalReport(http, type, content, definition, build, sessionId)
		}
	}

	// end session
	http.post {
		request.uri.path = "$teamscale.prefix/p/$teamscale.project/add-external-findings/$sessionId"
		request.contentType = "text/plain"
		response.failure failure
		setRequestTimeParameter(build, request)
	}
}

/**
 * Downloads the files for the given report type according to the artifact and file filter.
 * Returns file handles to files located in the /tmp/.
 * Delete the files if you're done with them!
 */
def getFilesWithFilter(http, options, definition, build) {
	def artifacts = build.artifacts.findAll { k, v -> k ==~ /${options.artifact}/ }

	assert artifacts.size() > 0: "No artifacts matched for the pattern: \"$options.artifact\""

	def files = artifacts.collectMany { name, id ->
		return http.get {
			request.uri.path = "$teamscale.prefix/_apis/resources/Containers/$id"
			request.uri.query = [
				"itemPath": name
			]
			response.failure(failure)
		}.value.collectMany {
			if (it.itemType == "file" && it.path ==~ /${options.file}/) {
				return [it.subMap("path", "contentLocation")]
			}
			return []
		}
	}

	assert files.size() < MAX_MATCHED_FILES: getLogMessage("'$options.file' matched ${files.size()} files. " +
		"Maximum allowed: $MAX_MATCHED_FILES", definition)

	// Download the files
	return files.collect { file ->
		return http.get {
			request.uri = file.contentLocation
			groovyx.net.http.optional.Download.toTempFile(delegate)
			response.failure(failure)
		}
	}
}

def log(message, definition = "", build = "") {
	println getLogMessage(message, definition, build)
}

def getLogMessage(message, definition = "", build = "") {
	if (definition) {
		build = build ? ":$build.details.buildNumber" : ""
		definition = definition ? "[$definition.name$build] " : ""
	}
	return definition + message
}

/**
 * Transforms the given report according to the defined transformation.
 * If no transformation is given, it simply returns the text of the report
 */
def transformReport(report, transformation) {
	switch (transformation) {
		case "VSTSCoverage":
			return transformVstsCoverage(report)
		case null:
			return report.text
		default:
			println "    -> transformation not found. Skipping"
			return null
	}
}

def transformVstsCoverage(coverage) {
	assert CODE_COVERAGE_EXE: "CODE_COVERAGE_EXE must be defined for the conversion of vsts coverage data"

	// Grabbing the output of the command has a ton of \u0000 characters; probably encoding problems
	def xml = new File("$rootDir/tmp/tmp.xml")
	def command = ["cmd", "/C", "\"\"$CODE_COVERAGE_EXE\"", "analyze", "/output:\"$xml.absolutePath\"",
				   "\"$coverage.absolutePath\"\""].execute()

	def errorStream = new StringBuffer()
	command.waitForProcessOutput(null, errorStream)

	if (errorStream) {
		log("    -> failed: ${errorStream.toString().trim()}")
		return null
	}

	def content = xml.text
	xml.delete()

	return content
}

// ### Tasks
/** Collects all build definitions for the project */
task collectBuildDefinitions {
	def defaultOptions = [
		// default values
		"branchMapping"    : { it },
		"uploadTestResults": true
	]

	doLast {
		vsts.builds.each { vstsUrl, vstsProjects ->
			vstsProjects.each { vstsProjectName, vstsBuilds ->
				def http = vsts.httpClient(vstsUrl, vsts.credentials[vstsUrl])

				def response = http.get {
					request.uri.path = "$teamscale.prefix/$vstsProjectName/_apis/build/definitions"
					request.uri.query = [
						'api-version'        : '4.1',
						'includeLatestBuilds': true
					]
					response.failure(failure)
				}

				def definitions = response.value.findAll { vstsBuildDefinition ->
					return vstsBuilds.containsKey(vstsBuildDefinition.name)
				}

				definitions.each { vstsBuildDefinition ->
					if (!vstsBuilds.containsKey(vstsBuildDefinition.name)) {
						return
					}

					def lastCompletedTime = Instant.EPOCH
					if (!vstsBuildDefinition.latestCompletedBuild) {
						log("No build run/completed for $vstsBuildDefinition.name")
					} else {
						lastCompletedTime = Instant.parse(vstsBuildDefinition.latestCompletedBuild.finishTime)
					}

					def buildDefinitionKey = [
						url    : vstsUrl,
						project: vstsProjectName,
						name   : vstsBuildDefinition.name,
						options: defaultOptions + vstsBuilds[vstsBuildDefinition.name]
					]

					buildDefinitions[buildDefinitionKey] = [
						id               : vstsBuildDefinition.id,
						lastCompletedTime: lastCompletedTime,
						http             : http,
						builds           : []
					]

					if (definitions.size() > 1) {
						assert buildDefinitionKey.options.partition: "The project '$buildDefinitionKey.project' " +
							"has ${definitions.size()} definitions, but there is no partition defined for " +
							"'$buildDefinitionKey.name'.\nIf there are multiple definitions for a project, " +
							"each one has to have a 'partition' property in its config gradle-file"
					}
				}
			}
		}
	}
}

/**
 * Collects all new builds for every build definition.
 * A build is new, if its timestamp is bigger than the last cached one.
 */
task collectNewBuilds(dependsOn: collectBuildDefinitions) {
	/**
	 * Checks if the build should be included based on the branchname.
	 * Pull request, for example, should automatically be excluded.
	 */
	def includeBuildByBranch = { branchName ->
		return !(branchName ==~ /^refs\/pull/)
	}

	/** Formats the branch to remove unneeded prefixes. */
	def formatBranchName = { branchName ->
		def gitRef = ~/^refs\/heads\//
		if (branchName.startsWith("\$")) {
			// TFS path will be ignored completely (for now)
			return ""
		}

		return branchName - gitRef
	}

	doLast {
		getCache()

		buildDefinitions.each { definition, data ->
			def cacheKey = "$definition.url/$definition.project/$definition.name"
			def lastProcessedCompletedTime = cache.lastProcessedCompletedTime[cacheKey]
			if (!lastProcessedCompletedTime) {
				lastProcessedCompletedTime = Instant.EPOCH
			} else {
				lastProcessedCompletedTime = Instant.parse(lastProcessedCompletedTime)
				if (data.lastCompletedTime <= lastProcessedCompletedTime) {
					log("No new builds since $lastProcessedCompletedTime", definition)
					return
				}
			}

			def response = data.http.get {
				request.uri.path = "$teamscale.prefix/$definition.project/_apis/build/builds"
				request.uri.query = [
					'api-version': '4.1',
					'definitions': data.id,
					'minTime'    : lastProcessedCompletedTime.plusNanos(1),
					'status'     : 'completed',
					'queryOrder' : 'finishTimeAscending',
				]
				response.failure(failure)
			}

			response.value.each { build ->
				def branchName = build.sourceBranch

				// Filter which builds should be included
				if (!includeBuildByBranch(branchName) || !["succeeded", "failed"].contains(build.result)) {
					return
				}

				branchName = formatBranchName(branchName)
				def targetBranch = definition.options.branchMapping(branchName)

				if (targetBranch instanceof String) {
					data.builds += [
						details     : build,
						targetBranch: targetBranch,
						queueTime   : Instant.parse(build.queueTime),
						findings    : new HashSet<Finding>(),
						artifacts   : [:],
						reports     : [:]
					]
				}
			}

			// TODO update step by step?
			cache.lastProcessedCompletedTime[cacheKey] = data.lastCompletedTime.toString()
		}
	}
}

/**
 * Fetches the test results for each build.
 * If the build does not run tests, the list is empty.
 */
task getTestResults(dependsOn: collectNewBuilds) {
	doLast {
		buildDefinitions.each { definition, definitionData ->
			if (!definition.options.uploadTestResults) {
				return
			}

			definitionData.builds.each { build ->
				def testRuns = definitionData.http.get {
					request.uri.path = "$teamscale.prefix/$definition.project/_apis/test/Runs"
					request.uri.query = [
						"api-version": "4.1",
						"builduri"   : "vstfs:///Build/Build/$build.details.id"
					]
					response.failure(failure)
				}

				def results = []
				testRuns.value.each { run ->
					results.add(definitionData.http.get {
						request.uri.path = "$teamscale.prefix/$definition.project/_apis/test/runs/$run.id"
						request.uri.query = [:]
					})
					response.failure(failure)
				}

				build.testResult = results
			}
		}
	}
}

/**
 * Fetches and parses the logs for each build. Every found finding is added to the build object in build definitions.
 *
 * Only the logs which actually contain warning are being parsed.
 * If a log is longer than #MAX_FETCHED_LOG_LINES it is fetched in batches
 */
task parseBuildLogs(dependsOn: collectNewBuilds) {
	/** Checks if the log in this record should be downloaded */
	def includeLogOfRecord = { definition, record ->
		if (!record.log || !(record.type ==~ /Task|Job/)) {
			return false
		}

		if (definition.options.taskNamePattern) {
			return (record.name ==~ definition.options.taskNamePattern)
		}

		// If we would include every task, take the job logs instead, as it contains the log of every
		// task, thus reducing the number of calls to the server
		return record.type == "Job"
	}

	/** Parses the given log with all specified analyzer */
	def parseLog = { log ->
		def findings = [] as Set
		vsts.loganalyzer.each { type ->
			def analyzer = vsts.availableLoganalyzer[type]
			log.eachLine { line ->
				if (line =~ analyzer.pattern) {
					findings += analyzer.convert(java.util.regex.Matcher.lastMatcher)
				}
			}
		}
		return findings
	}

	doLast {
		buildDefinitions.each { definition, definitionData ->
			if (definitionData.builds.size() > 0 && !definition.options.taskNamePattern) {
				log("WARNING: Option \"taskNamePattern\" is not defined, making this very inefficient. " +
					"See samples/vsts/project.gradle", definition)
			}

			definitionData.builds.each { build ->
				build.findings = [] as Set

				// Get the information of all logs (lineCount only exists here)
				def logs = definitionData.http.get {
					request.uri.path = "$teamscale.prefix/$definition.project/_apis/build/builds/$build.details.id/logs"
					request.uri.query = ['api-version': '4.1']
					response.failure(failure)
				}.value.groupBy { it.id }

				// Get the timeline ( for error- and warningCount )
				def timeline = definitionData.http.get {
					request.uri.path = "$teamscale.prefix/$definition.project/_apis/build/builds/$build.details.id/timeline"
					request.uri.query = ['api-version': '4.1']
					response.failure(failure)
				}

				// Get a map with id and linecount for logs
				def logRefs = timeline.records.collectMany {
					if (includeLogOfRecord(definition, it)) {
						return [it.log.subMap("id") +
									it.subMap("name") +
									// groupBy creates a list. We are grouping by id, so there always is only the one element
									logs[it.log.id][0].subMap("lineCount")]
					}
					return []
				}

				if (definition.options.taskNamePattern) {
					assert logRefs.size() > 0: "The pattern \"$definition.options.taskNamePattern\" does not match " +
						"any task, therefore no findings can be extracted from the logs"
				}

				log("Parsing the logs of the following tasks: $logRefs.name", definition, build)

				// Parse the selected job logs
				logRefs.each { logRef ->
					// Get Log in batches
					def currentLine = 0
					while (currentLine < logRef.lineCount) {
						def log = definitionData.http.get {
							request.uri.path = "$teamscale.prefix/$definition.project/_apis/build/builds/$build.details.id/logs/$logRef.id"
							request.uri.query = [
								"api-version": "4.1",
								"startLine"  : currentLine,
								"endLine"    : currentLine + MAX_FETCHED_LOG_LINES
							]
							response.failure(failure)
						}

						build.findings += parseLog(log)

						// endline is inclusiv. Therefore we need to add 1
						currentLine += MAX_FETCHED_LOG_LINES + 1
					}
				}
			}
		}
	}
}

/**
 * Takes the roslyn rules, defined in the files provided by vsts.availableRoslynReportRules and uploads them
 * to the teamscale service via the `custom-roslyn-findings`
 *
 * TODO (BETA): the service is not usable at the moment (15.06.18), because of a bug
 * https://jira.cqse.eu/browse/TS-15538
 */
task uploadRoslynRules() {
	doLast {
		// check cache
		getCache()

		// upload rules
		def diff = vsts.availableRoslynReportRules - cache.uploadedRoslynRules
		if (diff.size() > 0) {
			// get rules
			def http = teamscale.httpClient(teamscale)
			diff.each { rulesPath ->
				def rulesFile = new File("$rootDir/$rulesPath")
				def rulesJson = new JsonSlurper().parse(rulesFile)
				def report = new JsonBuilder(RoslynReport.getRulesReport(rulesJson)).toString()

				// Get Analysis Profile of project
				def analysisProfile = http.get {
					request.uri.path = "$teamscale.prefix/create-project/$teamscale.project"
					request.contentType = "text/xml;charset=utf-8"
					response.failure(failure)
				}.profile

				// upload
				log("Uploading '$rulesPath' to '$analysisProfile'")
				http.post {
					request.uri.path = "$teamscale.prefix/custom-roslyn-findings"
					request.contentType = "multipart/form-data"
					request.uri.query = [
						"analysis-profile": analysisProfile
					]

					request.body = groovyx.net.http.MultipartContent.multipart {
						field "report", report
					}

					request.encoder "multipart/form-data", groovyx.net.http.OkHttpEncoders.&multipart

					response.failure(failure)
					response.when("400") { response, reader ->
						if (reader.contains("You did not provide any reports") && report.size() > 0) {
							// Bug in the service. If the report was already uploaded, it reports an missing data error
							return "'$analysisProfile' already contains the rules in '$rulesPath'"
						} else {
							failure(response, reader)
						}
					}
				}

				cache.uploadedRoslynRules += rulesPath
			}
		}

		// set cache
		writeCache()
	}
}

/**
 * Converts the findings to a roslyn report and uploads it
 *
 * TODO (BETA): Depends on uploadRoslynRules, which is not working correctly at the moment
 * https://jira.cqse.eu/browse/TS-15538
 */
task uploadFindingsAsRoslynReport(dependsOn: parseBuildLogs) {
	doLast {
		def http = teamscale.httpClient(teamscale)
		buildDefinitions.each { definition, data ->
			data.builds.sort { it.queueTime }.each { build ->
				// TODO: Check if the findings are in fact C# Findings
				if (!build.findings) {
					return
				}

				// Create the report
				def report = new RoslynReport()
				report.addFindings(build.findings)
				report = new JsonBuilder(report).toString()

				// upload
				uploadExternalReports(http, ["roslyn": [report]], definition, build)
			}
		}
	}
}

/** Gets the information for each artifact of a build, such that that parts can be downloaded */
task getArtifacts(dependsOn: collectNewBuilds) {
	// Extracts the id of the artifact, which can be used to download it
	def extractContainerId = { artifact ->
		def match = artifact.resource.data =~ /^\#\/([0-9]+)\/${artifact.name}$/
		if (match.find()) {
			return match.group(1)
		}
		return false
	}

	doLast {
		def http = teamscale.httpClient(teamscale)
		buildDefinitions.each { definition, data ->
			data.builds.sort { it.queueTime }.each { build ->
				def artifacts = data.http.get {
					request.uri.path = "$teamscale.prefix/$definition.project/_apis/build/builds/$build.details.id/artifacts"
					request.uri.query = [
						"api-version": "4.1"
					]
					response.failure(failure)
				}.value.collectEntries {
					def id = extractContainerId(it)
					if (id) {
						return [it.name, id]
					}
					return []
				}

				build.artifacts = artifacts
			}
		}
	}
}

/**
 * Collects all external reports from teamscale.
 * Has the option to transform the reports before they can be uploaded with the `uploadExternalReports` task.
 * See the sample file at samples/vsts/project.gradle for a more detailed description.
 */
task collectExternalReports(dependsOn: getArtifacts) {
	doLast {
		def http = teamscale.httpClient(teamscale)
		buildDefinitions.each { definition, data ->
			// Check if reports are configured
			if (!definition.options.reports) {
				log("No reports configured", definition)
				return
			}

			data.builds.sort { it.queueTime }.each { build ->
				build.reports += definition.options.reports.collectEntries { reportType, options ->
					assert options.artifact && options.file: getLogMessage("Artifact and file regex must both " +
						"be defined for the \"$reportType\" report. Check the config", definition)
					def files = []
					try {
						files = getFilesWithFilter(data.http, options, definition, build)
						if (files.size() == 0) {
							log("Regex '$options.file' matched 0 files", definition, build)
							return [:]
						}

						if (options.transformation && files.size > 0) {
							log("Converting ${files.size()} $reportType report(s) with the " +
								"\"$options.transformation\" transformation", definition, build)
						}

						def contents = files.collect { file ->
							return transformReport(file, options.transformation)
						}
						contents.removeAll([null])

						if (contents.size() > 0) {
							return ["$reportType": contents]
						}
						return [:]
					} finally {
						files.each { file ->
							file.delete()
						}
					}
				}
			}
		}
	}
}

/**
 * Uploads external analysis tool reports (e.g. FindBugs) to teamscale.
 * This requires some configuration in order to make it work, see the sample file samples/vsts/project.gradle
 */
task uploadExternalReports(dependsOn: collectExternalReports) {
	doLast {
		def http = teamscale.httpClient(teamscale)
		buildDefinitions.each { definition, data ->
			data.builds.sort { it.queueTime }.each { build ->
				uploadExternalReports(http, build.reports, definition, build)
			}
		}
	}
}

/**
 * Uploads the test results for every definition to the teamscale server as a non-code metric.
 * If project has multiple definitions, each definition has to define a separate partition to which the
 * results will be uploaded to.
 */
task uploadTestResults(dependsOn: getTestResults) {
	doLast {
		def http = teamscale.httpClient(teamscale)
		buildDefinitions.each { definition, data ->
			if (!definition.options.uploadTestResults) {
				return
			}

			data.builds.sort { it.queueTime }.each { build ->
				def result = ["passed": 0, "total": 0, "time": 0]
				build.testResult.each { run ->
					result.passed += run.passedTests
					result.total += run.totalTests
					result.time += getExecutionTime(run.startedDate, run.completedDate)
				}

				if (build.details.result == "failed" && result.total == 0) {
					// Build probably failed before the tests could run
					log("Skipping failed build", definition, build)
					return
				}

				// Upload the result
				def partition = appendPartitionName("Test", ": ", definition)
				def path = appendPartitionName("Test Success", "/", definition)
				def message = "External Analysis ($partition): ${result.passed}/${result.total} passed"

				log("Uploading test results: $result", definition, build)
				http.put {
					request.uri.path = "$teamscale.prefix/p/$teamscale.project/add-non-code-metrics"
					request.contentType = "application/json"
					request.uri.query = [
						"skip-session": true,
						"message"     : message,
						"partition"   : partition
					]
					setRequestTimeParameter(build, request)

					request.body = [[
										"path"      : path,
										"content"   : "$result.passed/$result.total tests passed",
										"time"      : result.time,
										"assessment": [
											"GREEN": result.passed,
											"RED"  : result.total - result.passed
										]
									]]
					response.failure(failure)
				}
			}
		}
	}
}

/**
 * Uploads the build result for every definition to the teamscale server as a non-code metric.
 * If project has multiple definitions, each definition has to define a separate partition to which the
 * results will be uploaded to.
 */
task uploadBuildStatus(dependsOn: collectNewBuilds) {
	def buildResultMap = [
		"failed"   : [
			"assessment": "RED",
			"content"   : "Build is unstable"
		],
		"succeeded": [
			"assessment": "GREEN",
			"content"   : "Build is stable"
		]
	]

	doLast {
		def http = teamscale.httpClient(teamscale)

		buildDefinitions.each { definition, data ->
			data.builds.sort { it.queueTime }.each { build ->
				if (build.details.status != "completed") {
					return
				}

				def buildResult = buildResultMap[build.details.result]
				def partition = appendPartitionName("Build", ": ", definition)
				def message = "External Analysis ($partition): build $build.details.result"

				log("Uploading build status", definition, build)
				http.put {
					request.uri.path = "$teamscale.prefix/p/$teamscale.project/add-non-code-metrics"
					request.contentType = "application/json"
					request.uri.query = [
						"skip-session": true,
						"message"     : message,
						"partition"   : appendPartitionName("Build", ": ", definition)
					]
					setRequestTimeParameter(build, request)

					request.body = [[
										"path"      : appendPartitionName("Build Stability", "/", definition),
										"content"   : buildResult.content,
										"time"      : getExecutionTime(build.details.startTime, build.details.finishTime),
										"assessment": ["$buildResult.assessment": 1]
									]]

					response.failure(failure)
				}
			}
		}
	}
}

/**
 * Uploads the findings which are reported during the build to the teamscale server.
 * See #parseBuildLogs for more details on which findings are created.
 */
task uploadBuildFindings(dependsOn: parseBuildLogs) {
	doLast {
		def http = teamscale.httpClient(teamscale)
		buildDefinitions.each { definition, data ->
			data.builds.sort { it.queueTime }.each { build ->
				if (build.details.result == "failed" && build.findings.size() == 0) {
					log("Skipping failed build", definition, build)
					return
				}

				def partition = appendPartitionName("Findings", ": ", definition)

				// Upload findings
				log("Uploading ${build.findings.size()} finding(s)", definition, build)
				http.put {
					request.uri.path = "$teamscale.prefix/p/$teamscale.project/add-external-findings"
					request.contentType = 'application/json'
					request.uri.query = [
						"message"     : "External Analysis ($partition)",
						"partition"   : partition,
						"skip-session": true
					]
					setRequestTimeParameter(build, request)

					def fileFindings = build.findings.groupBy({ finding -> finding.path }).collect { k, v ->
						["path": k, "findings": v]
					}

					request.body = fileFindings
					response.failure(failure)
				}
			}
		}
	}
}

/** Task combining the upload of all possible data from a build */
task uploadVstsBuildData() {
	dependsOn uploadBuildStatus
	dependsOn uploadBuildFindings
	dependsOn uploadTestResults
	dependsOn uploadExternalReports

	doLast {
		writeCache()
	}
}
